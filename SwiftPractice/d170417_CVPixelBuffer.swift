//
//  d170417_CVPixelBuffer.swift
//  SwiftPractice
//
//  Created by yuki takei on 2017/04/17.
//  Copyright Â© 2017å¹´ Yuki Takei. All rights reserved.
//

/*
 å‚è€ƒ:
 SampleBuffer -> UIImage
 http://qiita.com/koki_h/items/91d9bf918df7c5788ffc
 
 http://qiita.com/hkato193/items/c0327e4c56ccf1c15e7d
 
 CoreImage>CIFilter
 http://qiita.com/kitanoow/items/2fd1c6dc415d7af769db
 
 
 UIImageã¨CGImageã®é•ã„
 http://anthrgrnwrld.hatenablog.com/entry/2016/08/05/124314
 
 [UIImage] <-> [CGImage] <-> [CIImage] In Swift
 http://qiita.com/imanishisatoshi/items/34d1e488d412cd23fd8d
 
 UIImage vs. CIImage vs. CGImage *******
 https://medium.com/@ranleung/uiimage-vs-ciimage-vs-cgimage-3db9d8b83d94
 
 Bitmap
 https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_images/dq_images.html
 
 memo
 
 CMSampleBuffer -> CVImageBufferRef -> CIImage -> CGImage -> UIImage

 
 Sample Code
     let imageData = UIImage.cgImage!.dataProvider!.data
     let data : UnsafePointer = CFDataGetBytePtr(imageData)
     let scale:CGFloat = 1.0//UIScreen.main.scale
     let address : Int = ((Int(self.size.width) * Int(pos.y * scale)) + Int(pos.x * scale)) * pixelDataByteSize
     
     let r = CGFloat(data[address])
     let g = CGFloat(data[address+1])
     let b = CGFloat(data[address+2])
     let a = CGFloat(data[address+3])

 
 CMSampleBuffer -> CVImageBufferRef -> CIImage -> CGImage -> UIImage
 -> CGImage -> ****** -> BytePointer(??) ->UnsafePointer(CFData)
 
 UIImage        <UIKit>
    >CGImage    <CoreGra>
    >CIImage    <CoreImage>   ç”»åƒåŠ å·¥
        >CIFilter
 
 <UIImage>
 ç‹é“ã€‚ç”Ÿã®ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆ
 é€šå¸¸ã€PNGã¾ãŸã¯JPEGè¡¨ç¾ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å«ã‚€NSDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã—ã€ãã‚Œã‚’UIImageã«å¤‰æ›ã™ã‚‹ã€‚
 
 <CIImage>
 CIImageã¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’è¡¨ã™ä¸å¤‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ãã‚Œã¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ã¿ãŒé–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã®æƒ…å ±ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
 
 <CGImage>
 CGImageã¯ãƒ“ãƒƒãƒˆãƒãƒƒãƒ—ã—ã‹è¡¨ç¾ã§ãã¾ã›ã‚“ã€‚ãƒ–ãƒ¬ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚„ãƒã‚¹ã‚­ãƒ³ã‚°ãªã©ã®CoreGraphicsã§ã®æ“ä½œã§ã¯ã€CGImageRefsãŒå¿…è¦ã§ã™ã€‚å®Ÿéš›ã®ãƒ“ãƒƒãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€CGImageã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ã¾ãŸã€NSBitmapImageRepsã«å¤‰æ›ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
 
 
 
 å‹•ç”»
 AVCaptureDevice.devices()      ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    ->AVCaptureSession          ã‚»ãƒƒã‚·ãƒ§ãƒ³çµŒç”±
        ->AVCaptureStillImageOutput()   ç”»åƒå–å¾—ï¼Ÿ
 
 AVCaptureVideoPreviewLayer.init(session: AVCaptureSession)
 self.view.layer.addSublayer(AVCaptureVideoPreviewLayer)
 
 Q:ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ‰±ã†ãƒ¡ãƒªãƒƒãƒˆ
 AVCaptureVideoPreviewLayer
 AVCaptureStillImageOutput ã“ã®ã‚¯ãƒ©ã‚¹ç¢ºèª
 
 
 UIImageã®é•·ã•ã‚„ä½ç½®ã®æŒ‡å®šã¯pointå˜ä½ã§è¡Œã†
 CGImageã®é•·ã•ã‚„ä½ç½®ã®æŒ‡å®šã¯pixelå˜ä½ã§è¡Œã†
    UIImageã‚’åŸºæº–ã«æŒ‡å®šã—ãŸRectã®x,y,width,heightã«ãã‚Œãã‚Œscaleã‚’ã‹ã‘ãŸRectã‚’æº–å‚™ã™ã‚‹
    //scale: 1pointå½“ãŸã‚Šã®pixelæ•°
    let scale = imageSize.height / viewSize.height
 
 
 iOS 10.0-
    AVCapturePhotoOutput æ¨å¥¨
    ã§ã‚‚ä½¿ã„ã«ãã„
 
 AVCapturePhotoOutput
 http://qiita.com/inoue0426/items/4f31e61a494eeb507881
 
 http://dev.classmethod.jp/smartphone/ios-10-avfoundation-takephoto/
 
 http://galakutaapp.blogspot.jp/2017/03/avcapturephotooutput.html
 
 ç”»åƒãŒå›è»¢ã™ã‚‹
 https://blog.isao.co.jp/auto_rotaion_image_from_ios/
 
*/


import UIKit
import AVFoundation

class d170417_CVPixelBuffer: UIViewController {
    
    
    
//    private var session: AVCaptureSession?
//    private var videoPreviewLayer: AVCaptureVideoPreviewLayer?
//    private var stillImageOutput: AVCapturePhotoOutput = AVCapturePhotoOutput()
//
//    
//    override func viewDidAppear(_ animated: Bool) {
//        session = AVCaptureSession()
//        session?.sessionPreset = AVCaptureSessionPresetPhoto
//        videoPreviewLayer = AVCaptureVideoPreviewLayer(session: session)
//        
//        guard let _videoPreviewLayer = videoPreviewLayer else { return }
//        
//        _videoPreviewLayer.masksToBounds = true
//        _videoPreviewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill
//        
////        videoPreviewLayer.layer.addSublayer(_videoPreviewLayer)
//        self.view.layer.addSublayer(_videoPreviewLayer)
//        
//        let device = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)
//        let input = try! AVCaptureDeviceInput(device: device)
//        session?.addInput(input)
//        
////        stillImageOutput = AVCapturePhotoOutput()
//        takePhoto()
//        session?.addOutput(stillImageOutput)
//        
//        
//    }
//    
//    func takePhoto() {
//        let settingsForMonitoring = AVCapturePhotoSettings()
//        
//        let settings = AVCapturePhotoSettings()
//        let previewPixelType = settings.availablePreviewPhotoPixelFormatTypes.first!
//        let previewFormat = [kCVPixelBufferPixelFormatTypeKey as String: previewPixelType,
//                             kCVPixelBufferWidthKey as String: 160,
//                             kCVPixelBufferHeightKey as String: 160]
//        settings.previewPhotoFormat = previewFormat
//        
//        
////        settingsForMonitoring.flashMode = .auto
////        settingsForMonitoring.isAutoStillImageStabilizationEnabled = true
////        settingsForMonitoring.isHighResolutionPhotoEnabled = false
//        
//        stillImageOutput.capturePhoto(with: settings, delegate: self)
//    }
    
    
//    @IBOutlet weak var imageView: UIImageView!
    
    var captureSesssion: AVCaptureSession!
    var stillImageOutput: AVCapturePhotoOutput!
    //    â†‘ æ›¸ãæ›ãˆ AVCaptureStillImageOutput â†’ AVCapturePhotoOutput
    
    func takePhoto(_ sender: Any){
        let settingsForMonitoring = AVCapturePhotoSettings()
        settingsForMonitoring.flashMode = .auto
        settingsForMonitoring.isAutoStillImageStabilizationEnabled = true
        settingsForMonitoring.isHighResolutionPhotoEnabled = false
        stillImageOutput?.capturePhoto(with: settingsForMonitoring, delegate: self)
    }
    //    AVCapturePhotoSettingsã¨ã„ã†æ–°ã—ã„ClassãŒAVCapturePhotoOutputã¨ä¸€ç·’ã«è¿½åŠ ã•ã‚ŒãŸã€‚
    //    ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãªã©ã®ç´°ã‹ã„è¨­å®šã¯AVCapturePhotoSettingsã§è¡Œã†
    
    override func viewDidLoad() {
        super.viewDidLoad()
        captureSesssion = AVCaptureSession()
        captureSesssion.sessionPreset = AVCaptureSessionPreset1920x1080
        stillImageOutput = AVCapturePhotoOutput()
        
        let device = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)
        
        //        ä»Šå›ã¯ğŸ“·ã®Frontã€Backã€Dualã®æŒ‡å®šã¯ã—ã¦ã„ãªã„ãŒã€ã™ã‚‹ã¨ã—ãŸã‚‰ã“ã‚“ãªæ„Ÿã˜
        //        do {
        //            var defaultVideoDevice: AVCaptureDevice?
        //            defaultVideoDevice = dualCameraDevice
        //        }
        //            else if let backCameraDevice = AVCaptureDevice.defaultDevice(withDeviceType: .builtInWideAngleCamera, mediaType: AVMediaTypeVideo, position: .back) {
        //                defaultVideoDevice = backCameraDevice
        //            }
        //            else if let frontCameraDevice = AVCaptureDevice.defaultDevice(withDeviceType: .builtInWideAngleCamera, mediaType: AVMediaTypeVideo, position: .front) {
        //               defaultVideoDevice = frontCameraDevice
        //            }
        //
        //            let videoDeviceInput = try AVCaptureDeviceInput(device: defaultVideoDevice)
        //
        //            if session.canAddInput(videoDeviceInput) {
        //                session.addInput(videoDeviceInput)
        //                self.videoDeviceInput = videoDeviceInput
        //
        //                DispatchQueue.main.async {
        //                    let statusBarOrientation = UIApplication.shared.statusBarOrientation
        //                    var initialVideoOrientation: AVCaptureVideoOrientation = .portrait
        //                    if statusBarOrientation != .unknown {
        //                        if let videoOrientation = statusBarOrientation.videoOrientation {
        //                            initialVideoOrientation = videoOrientation
        //                        }
        //                    }
        //
        //                    self.previewView.videoPreviewLayer.connection.videoOrientation = initialVideoOrientation
        //                }
        //            }
        //            else {
        //                setupResult = .configurationFailed
        //                session.commitConfiguration()
        //                return
        //            }
        //        }
        
        do {
            let input = try AVCaptureDeviceInput(device: device)
            if (captureSesssion.canAddInput(input)) {
                captureSesssion.addInput(input)
                if (captureSesssion.canAddOutput(stillImageOutput)) {
                    captureSesssion.addOutput(stillImageOutput)
                    captureSesssion.startRunning()
                    let captureVideoLayer: AVCaptureVideoPreviewLayer = AVCaptureVideoPreviewLayer.init(session: captureSesssion)
                    captureVideoLayer.frame = self.view.bounds
                    captureVideoLayer.videoGravity = AVLayerVideoGravityResizeAspectFill
                    self.view.layer.addSublayer(captureVideoLayer)
                }
            }
        }
        catch {
            print(error)
        }
    }

    
//AVCaptureVideoDataOutputSampleBufferDelegate
//    
//    var capDelegate:AVCaptureVideoDataOutputSampleBufferDelegate!
//    
//    // ã‚»ãƒƒã‚·ãƒ§ãƒ³.
//    var mySession : AVCaptureSession!
//    // ãƒ‡ãƒã‚¤ã‚¹.
//    var myDevice : AVCaptureDevice!
//    // ç”»åƒã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ.
//    var myImageOutput: AVCapturePhotoOutput!
//    
//    override func viewDidLoad() {
//        super.viewDidLoad()
//        
//        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ.
//        mySession = AVCaptureSession()
//        
//        
//        
//        // ãƒ‡ãƒã‚¤ã‚¹ä¸€è¦§ã®å–å¾—.
//        let devices = AVCaptureDevice.devices()
//        
//        // ãƒãƒƒã‚¯ã‚«ãƒ¡ãƒ©ã‚’myDeviceã«æ ¼ç´.
//        for device in devices! {
//            if((device as AnyObject).position == AVCaptureDevicePosition.back){
//                myDevice = device as! AVCaptureDevice
//            }
//        }
//        
//        // ãƒãƒƒã‚¯ã‚«ãƒ¡ãƒ©ã‹ã‚‰VideoInputã‚’å–å¾—.
//        let videoInput = try! AVCaptureDeviceInput.init(device: myDevice)
//        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¿½åŠ .
//        mySession.addInput(videoInput)
//        
//        // å‡ºåŠ›å…ˆã‚’ç”Ÿæˆ.
//        let settingsForMonitoring = AVCapturePhotoSettings()
//        settingsForMonitoring.flashMode = .auto
//        settingsForMonitoring.isAutoStillImageStabilizationEnabled = true
//        settingsForMonitoring.isHighResolutionPhotoEnabled = false
//        
//        myImageOutput = AVCapturePhotoOutput()
//        myImageOutput.capturePhoto(with: settingsForMonitoring, delegate: self)
//        
//        mySession?.sessionPreset = AVCaptureSessionPresetPhoto
//        mySession.addOutput(myImageOutput)
//        
////        // å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®å–å¾—.
////        var videoDataOutput:AVCaptureVideoDataOutput = AVCaptureVideoDataOutput()
////        // ã‚«ãƒ©ãƒ¼ãƒãƒ£ãƒ³ãƒãƒ«ã®è¨­å®š.
////        let dctPixelFormatType : Dictionary = [String(kCVPixelBufferPixelFormatTypeKey):kCVPixelFormatType_32BGRA]
////        //            let dctPixelFormatType : Dictionary = [kCVPixelBufferPixelFormatTypeKey : kCVPixelFormatType_32BGRA]
////        videoDataOutput.videoSettings = dctPixelFormatType
////        
////        // ãƒ‡ãƒªã‚²ãƒ¼ãƒˆã€ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ã‚­ãƒ¥ãƒ¼ã‚’æŒ‡å®š.
////        videoDataOutput.setSampleBufferDelegate(self, queue: DispatchQueue.main/*DispatchQueue.main*/)
////        
////        // ã‚­ãƒ¥ãƒ¼ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹ã¨ãã«æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ¥ãŸã‚‰å‰Šé™¤.
////        videoDataOutput.alwaysDiscardsLateVideoFrames = true
//        
//        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¿½åŠ .
////        mySession.addOutput(myImageOutput)
////        mySession.addOutput(videoDataOutput)
//        
//        
//        // ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ç”Ÿæˆ.
//        let myVideoLayer = AVCaptureVideoPreviewLayer.init(session: mySession)
//        myVideoLayer?.frame = self.view.bounds
//        myVideoLayer?.videoGravity = AVLayerVideoGravityResizeAspectFill
//        
//        // Viewã«è¿½åŠ .
//        self.view.layer.addSublayer(myVideoLayer!)
//        
//        // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹.
//        mySession.startRunning()
//        
//        
//        
//        // UIãƒœã‚¿ãƒ³ã‚’ä½œæˆ.
//        let myButton = UIButton(frame: CGRect(x: 0, y: 0, width: 120, height: 50))
//        myButton.backgroundColor = UIColor.red
//        myButton.layer.masksToBounds = true
//        myButton.setTitle("æ’®å½±", for: .normal)
//        myButton.layer.cornerRadius = 20.0
//        myButton.layer.position = CGPoint(x: self.view.bounds.width/2, y:self.view.bounds.height-50)
//        myButton.addTarget(self, action: #selector(onClickMyButton), for: .touchUpInside)
//        
//        // UIãƒœã‚¿ãƒ³ã‚’Viewã«è¿½åŠ .
//        self.view.addSubview(myButton);
//        
//    }
//    
//    // ãƒœã‚¿ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆ.
//    func onClickMyButton(sender: UIButton){
//        
//        // ãƒ“ãƒ‡ã‚ªå‡ºåŠ›ã«æ¥ç¶š.
//        // let myVideoConnection = myImageOutput.connectionWithMediaType(AVMediaTypeVideo)
//        let myVideoConnection = myImageOutput.connection(withMediaType: AVMediaTypeVideo)
//        
//        // æ¥ç¶šã‹ã‚‰ç”»åƒã‚’å–å¾—.
////        self.myImageOutput.captureStillImageAsynchronously(from: myVideoConnection, completionHandler: {(imageDataBuffer, error) in
////            if let e = error {
////                print(e.localizedDescription)
////                return
////            }
////            // å–å¾—ã—ãŸImageã®DataBufferã‚’Jpegã«å¤‰æ›.
////            let myImageData = AVCapturePhotoOutput.jpegPhotoDataRepresentation(forJPEGSampleBuffer: imageDataBuffer!, previewPhotoSampleBuffer: nil)
////            // Jpegã‹ã‚‰UIIMageã‚’ä½œæˆ.
////            let myImage = UIImage(data: myImageData!)
////            // ã‚¢ãƒ«ãƒãƒ ã«è¿½åŠ .
////            UIImageWriteToSavedPhotosAlbum(myImage!, nil, nil, nil)
////        })
//    }
//    
////    func captureOutput(_ captureOutput: AVCaptureOutput!, didOutputSampleBuffer sampleBuffer: CMSampleBuffer!, from connection: AVCaptureConnection!) {
////        
////        let pixelBuffer: CVPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)!
////        print("captureOutput")
////    }
//  
//    
    
}
//
extension d170417_CVPixelBuffer:AVCapturePhotoCaptureDelegate {
    
    func capture(_ captureOutput: AVCapturePhotoOutput, didFinishProcessingPhotoSampleBuffer photoSampleBuffer: CMSampleBuffer?, previewPhotoSampleBuffer: CMSampleBuffer?, resolvedSettings: AVCaptureResolvedPhotoSettings, bracketSettings: AVCaptureBracketedStillImageSettings?, error: Error?) {
        
        if let error = error {
            print(error.localizedDescription)
        }else{
            print("PhotoCap")
        }
        
        print("PhotoCap")
        
    }
}
